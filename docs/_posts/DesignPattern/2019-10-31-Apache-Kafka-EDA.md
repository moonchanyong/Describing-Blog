---
layout: post
title: Apache Kafka를 이용한 EDA 설계에서 고민한 기록
category: [ DesignPattern ]
tags: [ kafka ]
---

Created By: 찬용 문
Last Edited: 찬용 문

# Kafka를 이용하여 EDA 설계에서 고민한 기록

현재 프로젝트를 진행하면서 EDA기반으로 설계를 하며 스스로 고민이 되었던 부분이나 한번 고민을 하고 넘어가야 할 것 같은 부분은 기록 하기위해서 시작

## 설정

### Replication factor, Ack

운영 전이라 아직 크게 고민을 하지는 않았지만 짚고 넘어가야 하는 문제라고 생각한다. 보통 추천하는 설정은 replicationfactor 2나 3으로 한 후 다른 replica에 메시지의 복제가 일어나기 전에 leader가 다운 되는경우 메시지 유실을 방지하기위해서 Ack는 all로 설정을 한다. 현재 프로젝트의 도메인 내에서 메시지 유실에대해서 크게 영향이 있는지. replica까지의 복제가 다 완료되길 기다리는것이 모든 메시지에 적용이 된면서 리더 브로커에게만 받는거와 큰 차이가 있지 않을까 싶다.

우선 2, 3 그리고 ack는 -1로 설정을 해놓았다. 메시지 유실이 되면 결국 확인 후에 다시 요청을하여 가져올 계획이라 유실의 위험성이 적은 경우로 가기로 했다.

## 구현

### Exactly Once, At most Once, At least Once

쌉고수님의 설명을 듣고 고민해보고 인사이트를 받았다. cloud compiuting pattern에서 위의 Exactly Once, At most Once, At least Once의 설명이 나와있다. 간단하게 말하자면 메시지를 정확히 한번, 최대 한번, 최소 한번 보내는 것에 대한 고민이다.

[Cloud Computing Patterns](http://www.cloudcomputingpatterns.org/)

메시지 오퍼링 방법을 고민해 봐야하는 이유는 아래와 같다.

- 중복: 메시지를 소비하는 Consumer쪽에서 같은 메시지를 받아서 액션을 취하는 경우 멱등성이 보장이 안된다면 개발자의 의도와 다르게 흘러 갈 수 있기 때문이다.
- 유실: 메시지가 중간에 유실되어 Eventual consistency의 보장이 안된다. 정상적인 서비스의 동작을 기대 할 수 없다.

위와 같은이유로 고민을 하게되는데, 아래 cloud computing patterns 사이트에서 설명하는 각 메시지 오퍼링 방법과는 다르게  카프카는 더 심플 한 부분이 있다. at-most-once는 카프카 document의 `Message Delivery Semantics` 에 따르면 retries 설정을 0으로 하고 실패시에도 오프셋에대해 커밋을 하게 하면된다.

at-least-once는 기본적으로 카프카에서 보장을 해준다고하여 Exactly-once의 케이스에 대해서 생각해 봐야하는데 이 케이스 좀 번거롭다. 

1. [http://kafka.apache.org/documentation/#upgrade_11_exactly_once_semantics](http://kafka.apache.org/documentation/#upgrade_11_exactly_once_semantics)
    - 카프카 0.11.0부터 추가
    - idempotence 옵션 true
    - 카프카 버전 0.11.0의 메시지 포맷을 따라야한다.
    - 기타.. 추가로 생성된 internal 토픽에 대해서 설정을 한다.
    - 보안을 위해 .... 등등
2. Kafka Stream을 써라

Stream 최고.

## 배포

## [WIP] Consumer

막연하게 배포의 컨슈머라고 적었는데 카프카 컨슈머의 배포에 대해서 고민한다.

고민의 시작은 blue-green 배포를 하면서 파티션의 수를 어떻게 할까에 대한 고민이었다. 배포시마다 귀찮게 파티션의 수를 두배로 설정해줘야 하는 것과(이 때 잘못알고 파티션의 수를 줄일 수 있는줄 알았다.), 2배로 하지 않으면 Blue-Green을 의도적으로 못하지 않을까라는 고민이었다.

***마틴파울러의 블루그린 배포 정의***

[bliki: BlueGreenDeployment](https://martinfowler.com/bliki/BlueGreenDeployment.html)

마틴파울러가 예시를 든 것 처럼 블루를 기존에 떠있는것, Green을 새로 뜨는것으로 가정을 하자면 기존은 블루, 새로운 인스턴스는 그린에서 그린이 뜨는경우 기존애 파티션의 수는 블루에서 인스턴스 * 컨슈머 스레드만큼의 개수만큼 있다면 그린이 새로 컨슈머 그룹에 들어가면서 블루에서만 컨슘을 하고 있었는데 리밸런싱이 일어나서 그린에서도 컨슘을 하게 될 수도있다. 

여기까지만 하면 어차피 배포할건데 무슨상관인데? 라고 생각 할 수 있으나 그럴거면 왜 블루그린 배포를 하는가이다. 블루그린 배포를 하며 이득을 얻을 수 있는점이 카나리아 릴리즈와 빠른 롤백인데 롤백은 그린을 빠르게 끄므로 할 수 있다고 하면, 블루와 그린의 비율은 개발자가 의도적으로 조절 할 수없다. 조절을 하려면 그린의 인스턴스와 블루의 인스턴스의 수를 조절해야한다. 왜냐면 카프카 컨슈밍은 어떤 인스턴스가 할 지 모르기 때문이다.(쿠버환경에서는 서비스를 통해서 카나리아 릴리즈를 한다고 알고 있다. 쿠버가 아니라면 사내 환경)

이후 고민으로 아니면 애플리케이션 단에서 조절을 할 수 있도록 하면 어떨까라는 고민을 했는데 실제로 컨슈밍 하는 인스턴스만 라벨링을 한 후 headless 서비스 라벨셀렉터에서 이 라벨만 참조를하면 내가 띄운 애플리케이션에서 nslookup을 통해서 나 자신이 지금 서비스를 해도 되는 pod인가를 체크하는 로직을 넣으면 되지 않을까(아니면 현재 내가 속한 pod의 label을 알 수 있다면) 싶다.

다른 배포로는 롤링업데이트가 있는데 11번가의 발표에서 이는 리밸런싱에 대한 리소스 때문에 안하고 배포를 잘 하지않을 게이트웨이를 컨슈머로 두고 다른 서비스들이 게이트웨이에서 메시지를 받아 사용하는 형태로 하였다.

아직 배포전략을 수립하지 않고 고민에대해서만 적어서 이후 추가 될 수도 있다.